{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lt50pljzqMCL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
        "from tensorflow.keras.models import Model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YS6N7IqbsXXL",
        "outputId": "bd9eaab6-3837-4454-f778-d8ed0b77bdda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q \"/content/drive/My Drive/Hand Gesture Data.zip\" -d /content/\n"
      ],
      "metadata": {
        "id": "8006Zc9bseui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator = train_datagen.flow_from_directory(\n",
        "    '/content/Rock-Paper-Scissors',\n",
        "    target_size=(300, 300),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    '/content/Rock-Paper-Scissors',\n",
        "    target_size=(300, 300),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35G7Nq1ssuPS",
        "outputId": "4290fd84-b8e1-4c39-ffcb-0423ec3e28d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2341 images belonging to 3 classes.\n",
            "Found 584 images belonging to 3 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = MobileNetV2(\n",
        "    input_shape=(300, 300, 3),\n",
        "    include_top=False,\n",
        "    weights='imagenet'\n",
        ")\n",
        "\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "predictions = Dense(3, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubq0QV4au_v0",
        "outputId": "b2339a33-a46f-4d20-c5c3-c70af88c0eaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "9406464/9406464 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=10,\n",
        "    validation_data=validation_generator\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frBldkV2vLZy",
        "outputId": "eac57097-83fd-46f9-c1cd-da42347afb51"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "74/74 [==============================] - 1198s 16s/step - loss: 0.1445 - accuracy: 0.9543 - val_loss: 14.5659 - val_accuracy: 0.1267\n",
            "Epoch 2/10\n",
            "74/74 [==============================] - 1169s 16s/step - loss: 0.0687 - accuracy: 0.9791 - val_loss: 3.2164 - val_accuracy: 0.8630\n",
            "Epoch 3/10\n",
            "74/74 [==============================] - 1164s 16s/step - loss: 0.0200 - accuracy: 0.9949 - val_loss: 2.3119 - val_accuracy: 0.8630\n",
            "Epoch 4/10\n",
            "74/74 [==============================] - 1172s 16s/step - loss: 0.0218 - accuracy: 0.9940 - val_loss: 4.6755 - val_accuracy: 0.8630\n",
            "Epoch 5/10\n",
            "74/74 [==============================] - 1139s 15s/step - loss: 0.0335 - accuracy: 0.9923 - val_loss: 7.2548 - val_accuracy: 0.8630\n",
            "Epoch 6/10\n",
            "74/74 [==============================] - 1148s 16s/step - loss: 0.0283 - accuracy: 0.9906 - val_loss: 34.5608 - val_accuracy: 0.1267\n",
            "Epoch 7/10\n",
            "74/74 [==============================] - 1132s 15s/step - loss: 0.0152 - accuracy: 0.9949 - val_loss: 18.7020 - val_accuracy: 0.1267\n",
            "74/74 [==============================] - 1132s 15s/step - loss: 0.0152 - accuracy: 0.9949 - val_loss: 18.7020 - val_accuracy: 0.1267\n",
            "Epoch 8/10\n",
            "Epoch 8/10\n",
            "74/74 [==============================] - 1130s 15s/step - loss: 0.0086 - accuracy: 0.9970 - val_loss: 20.7042 - val_accuracy: 0.1284\n",
            "74/74 [==============================] - 1130s 15s/step - loss: 0.0086 - accuracy: 0.9970 - val_loss: 20.7042 - val_accuracy: 0.1284\n",
            "Epoch 9/10\n",
            "Epoch 9/10\n",
            "74/74 [==============================] - 1101s 15s/step - loss: 0.0228 - accuracy: 0.9932 - val_loss: 1.4813 - val_accuracy: 0.8116\n",
            "74/74 [==============================] - 1101s 15s/step - loss: 0.0228 - accuracy: 0.9932 - val_loss: 1.4813 - val_accuracy: 0.8116\n",
            "Epoch 10/10\n",
            "Epoch 10/10\n",
            "74/74 [==============================] - 1109s 15s/step - loss: 0.0240 - accuracy: 0.9940 - val_loss: 20.2028 - val_accuracy: 0.1267\n",
            "74/74 [==============================] - 1109s 15s/step - loss: 0.0240 - accuracy: 0.9940 - val_loss: 20.2028 - val_accuracy: 0.1267\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/hand_gesture_model.h5')\n"
      ],
      "metadata": {
        "id": "bZAzP4w1bg80"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}